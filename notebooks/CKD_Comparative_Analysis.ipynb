{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcdc436",
   "metadata": {},
   "source": [
    "# CKD Risk Prediction: Comparative Analysis\n",
    "## Real-Only Model vs. Real + Synthetic Augmented Model\n",
    "\n",
    "This notebook compares:\n",
    "1. **Real Data Only** (pretrained models: LR, RF, XGB)\n",
    "2. **Real + Synthetic Data Combined** (newly trained augmented models)\n",
    "\n",
    "Primary reporting in this notebook uses **5-fold cross-validation (CV)**:\n",
    "- Summary table uses **CV mean ± std** metrics\n",
    "- Plots use **CV out-of-fold predictions** (ROC + confusion matrices)\n",
    "\n",
    "A **purely real test set** is still loaded for consistency/optional checks, but plots and the main comparison focus on CV metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### Analysis Goals\n",
    "- Load pretrained model trained on real data\n",
    "- Train a new model on augmented data (real + synthetic)\n",
    "- Compare **CV performance metrics** between scenarios\n",
    "- Generate visualizations (CV metrics, CV ROC curves, CV confusion matrices)\n",
    "- Print and save a comparative CV metrics table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b84f2c",
   "metadata": {},
   "source": [
    "## 1. Imports, Paths, and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    make_scorer,\n",
    " )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional dependency (only needed if you run XGB)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    XGBClassifier = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b18c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'X_train_preproc_path': '../data/processed/preprocessed/X_train_preproc.csv',\n",
    "    'X_test_preproc_path': '../data/processed/preprocessed/X_test_preproc.csv',\n",
    "    'X_synth_path': '../data/synthetic/X_synth_3x_gcopula_preproc.csv',\n",
    "    'X_train_raw_path': '../data/processed/splits/X_train_raw.csv',\n",
    "    'X_test_raw_path': '../data/processed/splits/X_test_raw.csv',\n",
    "\n",
    "    # Label paths\n",
    "    'y_train_path': '../data/processed/splits/y_train.csv',\n",
    "    'y_test_path': '../data/processed/splits/y_test.csv',\n",
    "    'y_synth_path': '../data/synthetic/y_synth_3x_gcopula.csv',\n",
    "\n",
    "    # Pretrained model paths (Real Only models)\n",
    "    'model_lr_real_path': '../models/lr.joblib',\n",
    "    'model_rf_real_path': '../models/rf.joblib',\n",
    "    'model_xgb_real_path': '../models/xgb.joblib',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT SETTINGS\n",
    "# ============================================================================\n",
    "MODEL_NAMES = ['lr', 'rf', 'xgb']  # extendable\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Models to run: {[m.upper() for m in MODEL_NAMES]}\")\n",
    "print(f\"CV folds: {CV_FOLDS}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2769e1d",
   "metadata": {},
   "source": [
    "## 2. Load Real Train & Test Features + Labels\n",
    "(Test set remains purely real; main comparison/plots use CV metrics.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_y_csv(path: str | None) -> pd.Series:\n",
    "    if not path:\n",
    "        raise ValueError(\n",
    "            \"y path not provided. Set CONFIG['y_train_path'] / CONFIG['y_test_path'] / CONFIG['y_synth_path'].\"\n",
    "        )\n",
    "\n",
    "    # Robust read: handle both headerless single-column CSV and CSV with a header.\n",
    "    y_df = pd.read_csv(path, header=None)\n",
    "    if y_df.shape[1] == 0:\n",
    "        raise ValueError(f\"No columns found in label file: {path}\")\n",
    "\n",
    "    y = y_df.iloc[:, 0]\n",
    "\n",
    "    # Drop a header-like first row if present\n",
    "    if y.dtype == object and len(y) > 0:\n",
    "        first = str(y.iloc[0]).strip().lower()\n",
    "        if first in {'label', 'labels', 'target', 'y', 'class', 'classification'}:\n",
    "            y = y.iloc[1:]\n",
    "\n",
    "    return y.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def normalize_binary_y(y: pd.Series) -> pd.Series:\n",
    "    y = y.copy()\n",
    "\n",
    "    # If it's already numeric 0/1, keep it.\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        unique_vals = set(pd.unique(y.dropna()))\n",
    "        if unique_vals.issubset({0, 1}):\n",
    "            return y.astype(int)\n",
    "        return y\n",
    "\n",
    "    # Otherwise map common CKD label strings\n",
    "    y_str = y.astype(str).str.strip().str.lower()\n",
    "    mapping = {\n",
    "        'ckd': 1,\n",
    "        'notckd': 0,\n",
    "        'yes': 1,\n",
    "        'no': 0,\n",
    "        '1': 1,\n",
    "        '0': 0,\n",
    "        'true': 1,\n",
    "        'false': 0,\n",
    "    }\n",
    "    y_mapped = y_str.map(mapping)\n",
    "    if y_mapped.isna().any():\n",
    "        bad = sorted(set(y_str[y_mapped.isna()].unique()))\n",
    "        raise ValueError(\n",
    "            \"Unrecognized label values in y. Provide numeric 0/1 labels or update mapping. \"\n",
    "            f\"Bad values: {bad[:10]}\"\n",
    "        )\n",
    "    return y_mapped.astype(int)\n",
    "\n",
    "\n",
    "def read_x_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Only drop completely empty rows (common when CSV has trailing newlines)\n",
    "    return df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# The exact 6 features your models expect (RAW feature names)\n",
    "FEAT_6 = ['hemo', 'sc', 'al', 'htn', 'age', 'dm']\n",
    "\n",
    "# 1) Load RAW train/test data for the pretrained pipeline (Scenario A)\n",
    "X_train_raw = read_x_csv(CONFIG['X_train_raw_path'])\n",
    "X_test_raw = read_x_csv(CONFIG['X_test_raw_path'])\n",
    "print(f\"Loaded X_train_raw: {X_train_raw.shape}\")\n",
    "print(f\"Loaded X_test_raw:  {X_test_raw.shape}\")\n",
    "\n",
    "# 2) Load PREPROCESSED features for the augmented model (Scenario B)\n",
    "X_train_preproc = read_x_csv(CONFIG['X_train_preproc_path'])\n",
    "X_test_preproc = read_x_csv(CONFIG['X_test_preproc_path'])\n",
    "print(f\"Loaded X_train_preproc: {X_train_preproc.shape}\")\n",
    "print(f\"Loaded X_test_preproc:  {X_test_preproc.shape}\")\n",
    "\n",
    "# Labels (shared ground-truth; test set remains purely real)\n",
    "y_train = normalize_binary_y(read_y_csv(CONFIG.get('y_train_path')))\n",
    "y_test = normalize_binary_y(read_y_csv(CONFIG.get('y_test_path')))\n",
    "\n",
    "# Basic alignment checks\n",
    "if len(X_train_preproc) != len(y_train):\n",
    "    raise ValueError(f\"X_train_preproc and y_train length mismatch: {len(X_train_preproc)} vs {len(y_train)}\")\n",
    "if len(X_train_raw) != len(y_train):\n",
    "    raise ValueError(f\"X_train_raw and y_train length mismatch: {len(X_train_raw)} vs {len(y_train)}\")\n",
    "if len(X_test_preproc) != len(y_test):\n",
    "    raise ValueError(f\"X_test_preproc and y_test length mismatch: {len(X_test_preproc)} vs {len(y_test)}\")\n",
    "\n",
    "if not set(FEAT_6).issubset(set(X_train_raw.columns)):\n",
    "    missing = sorted(set(FEAT_6) - set(X_train_raw.columns))\n",
    "    raise ValueError(f\"X_train_raw is missing required FEAT_6 columns: {missing}\")\n",
    "if not set(FEAT_6).issubset(set(X_test_raw.columns)):\n",
    "    missing = sorted(set(FEAT_6) - set(X_test_raw.columns))\n",
    "    raise ValueError(f\"X_test_raw is missing required FEAT_6 columns: {missing}\")\n",
    "\n",
    "print(\"Data loaded. Test sets are purely real (raw + preprocessed views).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645d554",
   "metadata": {},
   "source": [
    "## 3. Load Synthetic Features + Labels (for Augmented Training Only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e04596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic PREPROCESSED features (training only)\n",
    "X_synth = pd.read_csv(CONFIG['X_synth_path'])\n",
    "print(f\"Loaded X_synth: {X_synth.shape}\")\n",
    "\n",
    "# Synthetic labels\n",
    "y_synth = normalize_binary_y(read_y_csv(CONFIG.get('y_synth_path')))\n",
    "\n",
    "if len(X_synth) != len(y_synth):\n",
    "    raise ValueError(f\"X_synth and y_synth length mismatch: {len(X_synth)} vs {len(y_synth)}\")\n",
    "\n",
    "print(\"Synthetic training data loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05ed7",
   "metadata": {},
   "source": [
    "## 4. Schema & Column Alignment Checks (Real vs. Synthetic vs. Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aff906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking column consistency for PREPROCESSED data...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CRITICAL FIX: Ensure columns match exactly (preproc real vs preproc synth)\n",
    "# ---------------------------------------------------------\n",
    "common_cols = X_train_preproc.columns.intersection(X_synth.columns)\n",
    "\n",
    "if len(common_cols) == 0:\n",
    "    raise ValueError(\"No overlapping columns between X_train_preproc and X_synth.\")\n",
    "\n",
    "X_train_preproc = X_train_preproc[common_cols]\n",
    "X_synth = X_synth[common_cols]\n",
    "X_test_preproc = X_test_preproc[common_cols]\n",
    "\n",
    "print(f\"Aligned preprocessed columns: {len(common_cols)}\")\n",
    "\n",
    "# Map FEAT_6 (raw names) to the preprocessed equivalents\n",
    "# In your preprocessed snapshot, binary features are one-hot encoded as *_0.0 / *_1.0.\n",
    "FEAT_6_PREPROC = ['hemo', 'sc', 'al', 'age', 'htn_1.0', 'dm_1.0']\n",
    "\n",
    "missing_preproc = sorted(set(FEAT_6_PREPROC) - set(X_train_preproc.columns))\n",
    "if missing_preproc:\n",
    "    raise ValueError(\n",
    "        \"Preprocessed datasets are missing required columns for FEAT_6_PREPROC. \"\n",
    "        f\"Missing: {missing_preproc}\"\n",
    "    )\n",
    "\n",
    "# Subset to the 6-feature set for the augmented model (to match the scenario)\n",
    "X_train_preproc_6 = X_train_preproc[FEAT_6_PREPROC]\n",
    "X_test_preproc_6 = X_test_preproc[FEAT_6_PREPROC]\n",
    "X_synth_6 = X_synth[FEAT_6_PREPROC]\n",
    "\n",
    "print(\"Checking for missing/invalid values...\")\n",
    "for name, df in [\n",
    "    ('X_train_preproc_6', X_train_preproc_6),\n",
    "    ('X_test_preproc_6', X_test_preproc_6),\n",
    "    ('X_synth_6', X_synth_6),\n",
    "]:\n",
    "    nan_count = int(df.isna().sum().sum())\n",
    "    inf_count = int(np.isinf(df).sum().sum())\n",
    "    if nan_count or inf_count:\n",
    "        print(f\"{name}: NaNs={nan_count}, infs={inf_count} (expected: none for preprocessed data)\")\n",
    "    else:\n",
    "        print(f\"{name}: OK\")\n",
    "\n",
    "print(\"Checking numeric dtypes...\")\n",
    "non_numeric = {\n",
    "    'X_train_preproc_6': X_train_preproc_6.select_dtypes(exclude=[np.number]).columns.tolist(),\n",
    "    'X_test_preproc_6': X_test_preproc_6.select_dtypes(exclude=[np.number]).columns.tolist(),\n",
    "    'X_synth_6': X_synth_6.select_dtypes(exclude=[np.number]).columns.tolist(),\n",
    "}\n",
    "\n",
    "bad = {k: v for k, v in non_numeric.items() if v}\n",
    "if bad:\n",
    "    raise ValueError(f\"Non-numeric columns found (expected numeric-only): {bad}\")\n",
    "\n",
    "# Raw test sanity check (Scenario A)\n",
    "if not set(FEAT_6).issubset(set(X_test_raw.columns)):\n",
    "    missing = sorted(set(FEAT_6) - set(X_test_raw.columns))\n",
    "    raise ValueError(f\"X_test_raw missing FEAT_6 columns: {missing}\")\n",
    "\n",
    "print(\"Schema checks complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0102c4",
   "metadata": {},
   "source": [
    "## 5. Load Pretrained \"Real Only\" Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec14905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    'lr': CONFIG['model_lr_real_path'],\n",
    "    'rf': CONFIG['model_rf_real_path'],\n",
    "    'xgb': CONFIG['model_xgb_real_path'],\n",
    "}\n",
    "\n",
    "# Load all pretrained (real-only) models requested\n",
    "model_real_by_name = {}\n",
    "X_test_raw_6 = X_test_raw[FEAT_6]\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    if model_name not in model_paths:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported. Available: {sorted(model_paths.keys())}\")\n",
    "\n",
    "    model_path = model_paths[model_name]\n",
    "    print(f\"Loading pretrained {model_name.upper()} model (trained on real data only)...\")\n",
    "    model_real = joblib.load(model_path)\n",
    "    model_real_by_name[model_name] = model_real\n",
    "    print(f\"  Loaded from: {model_path}\")\n",
    "\n",
    "    # Validate model using RAW test data (Scenario A)\n",
    "    try:\n",
    "        _ = model_real.predict(X_test_raw_6)\n",
    "        print(\"  OK: predicts on X_test_raw\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Pretrained {model_name.upper()} model validation failed on raw input. \"\n",
    "            \"This indicates the saved model is not compatible with the raw input schema. \"\n",
    "            f\"Error: {e}\"\n",
    "        )\n",
    "\n",
    "print(\"All pretrained models loaded and validated on raw input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a7ab",
   "metadata": {},
   "source": [
    "## 6. Build Augmented Training Set (Real + Synthetic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build augmented training set using the aligned PREPROCESSED 6-feature view\n",
    "if X_train_preproc_6 is None or X_synth_6 is None:\n",
    "\traise ValueError(\"X_train_preproc_6 or X_synth_6 is None. Check data loading and preprocessing steps.\")\n",
    "if y_train is None or y_synth is None:\n",
    "\traise ValueError(\"y_train or y_synth is None. Check label loading and preprocessing steps.\")\n",
    "\n",
    "X_parts = [X_train_preproc_6, X_synth_6]\n",
    "y_parts = [y_train, y_synth]\n",
    "\n",
    "X_aug = pd.concat(X_parts, axis=0, ignore_index=True)\n",
    "y_aug = pd.concat(y_parts, axis=0, ignore_index=True)\n",
    "\n",
    "# Ensure labels are a clean 1D Series\n",
    "y_aug = pd.Series(y_aug).reset_index(drop=True)\n",
    "\n",
    "print(\"Augmented training set created:\")\n",
    "print(f\"- Real (preproc) samples: {len(X_train_preproc_6)}\")\n",
    "print(f\"- Synthetic (preproc) samples: {len(X_synth_6)}\")\n",
    "print(f\"- Total augmented samples: {len(X_aug)}\")\n",
    "print(f\"- X_aug: {X_aug.shape}, y_aug: {y_aug.shape}\")\n",
    "\n",
    "X_aug, y_aug = shuffle(X_aug, y_aug, random_state=RANDOM_STATE)\n",
    "print(f\"Augmented dataset shuffled (random_state={RANDOM_STATE}).\")\n",
    "\n",
    "print(\"\\nConstraint check: test sets remain purely real (not augmented with synthetic data).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3555d11",
   "metadata": {},
   "source": [
    "## 7. Train Augmented Model (Real + Synthetic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmented_estimator(MODEL_NAME: str):\n",
    "    \"\"\"Create a fresh estimator for training on PREPROCESSED 6-feature data.\"\"\"\n",
    "    # When training model_augmented (Real + Synthetic), use EXACTLY these params:\n",
    "    if MODEL_NAME == 'lr':\n",
    "        return  LogisticRegression(\n",
    "            solver='liblinear', \n",
    "            C=1.0, \n",
    "            max_iter=1000, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif MODEL_NAME == 'rf':\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=7,  \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif MODEL_NAME == 'xgb':\n",
    "        if XGBClassifier is None:\n",
    "            raise ImportError(\"xgboost is not installed. Run: %pip install xgboost\")\n",
    "        return XGBClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=5,  \n",
    "            learning_rate=0.05,\n",
    "            random_state=RANDOM_STATE,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {MODEL_NAME}\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _unique_path(path: Path) -> Path:\n",
    "    if not path.exists():\n",
    "        return path\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = path.with_name(f\"{path.stem}_v{i}{path.suffix}\")\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "print(\"Training augmented models on Real + Synthetic PREPROCESSED data...\")\n",
    "model_augmented_by_name = {}\n",
    "\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "tag = f\"augmented_{RANDOM_STATE}\"\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    model = make_augmented_estimator(model_name)\n",
    "    model.fit(X_aug, y_aug)\n",
    "    model_augmented_by_name[model_name] = model\n",
    "    print(f\"- {model_name.upper()}: trained on {len(X_aug)} rows.\")\n",
    "\n",
    "    out_path = _unique_path(models_dir / f\"{model_name}_{tag}.joblib\")\n",
    "    joblib.dump(model, out_path)\n",
    "    print(f\"  Saved: {out_path}\")\n",
    "\n",
    "# Also save the whole dict as a single artifact (optional but convenient)\n",
    "bundle_path = _unique_path(models_dir / f\"models_{tag}.joblib\")\n",
    "joblib.dump(model_augmented_by_name, bundle_path)\n",
    "print(f\"Saved model bundle: {bundle_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81efbc2",
   "metadata": {},
   "source": [
    "## 8. Cross-Validated Evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running 5-fold cross-validation for each model (metrics + CV-based ROC/confusion)...\\n\")\n",
    "\n",
    "SCENARIOS = ['Real Only', 'Real + Synthetic']\n",
    "\n",
    "# CV scoring (explicit to ensure zero_division=0 where applicable)\n",
    "SCORING = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc',\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Data views for CV\n",
    "X_train_raw_6 = X_train_raw[FEAT_6]\n",
    "\n",
    "def summarize_cv(cv_out: dict) -> dict:\n",
    "    out = {}\n",
    "    for k, v in cv_out.items():\n",
    "        if not k.startswith('test_'):\n",
    "            continue\n",
    "        metric_name = k.replace('test_', '')\n",
    "        out[f'{metric_name}_mean'] = float(np.mean(v))\n",
    "        out[f'{metric_name}_std'] = float(np.std(v, ddof=1)) if len(v) > 1 else 0.0\n",
    "    return out\n",
    "\n",
    "def _oof_score(estimator, X: pd.DataFrame, y: pd.Series, cv_obj) -> np.ndarray:\n",
    "    \"\"\"Out-of-fold continuous scores for ROC-AUC/ROC curve.\"\"\"\n",
    "    try:\n",
    "        proba = cross_val_predict(estimator, X, y, cv=cv_obj, method='predict_proba', n_jobs=-1)\n",
    "        proba = np.asarray(proba)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "        return proba.ravel()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        scores = cross_val_predict(estimator, X, y, cv=cv_obj, method='decision_function', n_jobs=-1)\n",
    "        return np.asarray(scores).ravel()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    y_pred = cross_val_predict(estimator, X, y, cv=cv_obj, method='predict', n_jobs=-1)\n",
    "    return np.asarray(y_pred).astype(float).ravel()\n",
    "\n",
    "def _oof_labels(estimator, X: pd.DataFrame, y: pd.Series, cv_obj) -> np.ndarray:\n",
    "    return np.asarray(cross_val_predict(estimator, X, y, cv=cv_obj, method='predict', n_jobs=-1)).ravel()\n",
    "\n",
    "def evaluate_oof(y_true: pd.Series, y_pred: np.ndarray, y_score: np.ndarray) -> dict:\n",
    "    out = {\n",
    "        'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'y_pred': y_pred,\n",
    "        'y_score': y_score,\n",
    "    }\n",
    "    try:\n",
    "        out['roc_auc'] = float(roc_auc_score(y_true, y_score))\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        out['roc_curve'] = {'fpr': fpr, 'tpr': tpr}\n",
    "    except Exception:\n",
    "        out['roc_auc'] = float('nan')\n",
    "        out['roc_curve'] = {'fpr': np.array([0.0, 1.0]), 'tpr': np.array([0.0, 1.0])}\n",
    "    return out\n",
    "\n",
    "# Outputs\n",
    "cv_results = {}   # mean/std from cross_validate\n",
    "oof_results = {}  # CV-based evaluation from out-of-fold predictions\n",
    "roc_data = {}     # CV-based ROC curve data (from out-of-fold predictions)\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"{'='*80}\\nMODEL: {model_name.upper()}\\n{'='*80}\")\n",
    "\n",
    "    # --------------------\n",
    "    # Real Only (CV on real training data)\n",
    "    # --------------------\n",
    "    est_real = clone(model_real_by_name[model_name])\n",
    "    real_cv_out = cross_validate(\n",
    "        est_real,\n",
    "        X_train_raw_6,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        scoring=SCORING,\n",
    "        n_jobs=-1,\n",
    "        error_score='raise',\n",
    "    )\n",
    "    real_cv_sum = summarize_cv(real_cv_out)\n",
    "    cv_results[(model_name, 'Real Only')] = real_cv_sum\n",
    "\n",
    "    y_pred_real = _oof_labels(clone(model_real_by_name[model_name]), X_train_raw_6, y_train, cv)\n",
    "    y_score_real = _oof_score(clone(model_real_by_name[model_name]), X_train_raw_6, y_train, cv)\n",
    "    real_oof = evaluate_oof(y_train, y_pred_real, y_score_real)\n",
    "    oof_results[(model_name, 'Real Only')] = real_oof\n",
    "    roc_data[(model_name, 'Real Only')] = {\n",
    "        'fpr': real_oof['roc_curve']['fpr'],\n",
    "        'tpr': real_oof['roc_curve']['tpr'],\n",
    "        'auc': real_oof['roc_auc'],\n",
    "    }\n",
    "\n",
    "    print(\"5-fold CV metrics (Real Only, on REAL training data):\")\n",
    "    print(\n",
    "        f\"  Mean±Std | Acc={real_cv_sum['accuracy_mean']:.4f}±{real_cv_sum['accuracy_std']:.4f}  \"\n",
    "        f\"Prec={real_cv_sum['precision_mean']:.4f}±{real_cv_sum['precision_std']:.4f}  \"\n",
    "        f\"Rec={real_cv_sum['recall_mean']:.4f}±{real_cv_sum['recall_std']:.4f}  \"\n",
    "        f\"F1={real_cv_sum['f1_mean']:.4f}±{real_cv_sum['f1_std']:.4f}  \"\n",
    "        f\"AUC={real_cv_sum['roc_auc_mean']:.4f}±{real_cv_sum['roc_auc_std']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # --------------------\n",
    "    # Real + Synthetic (CV on augmented training data)\n",
    "    # --------------------\n",
    "    est_aug = make_augmented_estimator(model_name)\n",
    "    aug_cv_out = cross_validate(\n",
    "        est_aug,\n",
    "        X_aug,\n",
    "        y_aug,\n",
    "        cv=cv,\n",
    "        scoring=SCORING,\n",
    "        n_jobs=-1,\n",
    "        error_score='raise',\n",
    "    )\n",
    "    aug_cv_sum = summarize_cv(aug_cv_out)\n",
    "    cv_results[(model_name, 'Real + Synthetic')] = aug_cv_sum\n",
    "\n",
    "    y_pred_aug = _oof_labels(make_augmented_estimator(model_name), X_aug, y_aug, cv)\n",
    "    y_score_aug = _oof_score(make_augmented_estimator(model_name), X_aug, y_aug, cv)\n",
    "    aug_oof = evaluate_oof(y_aug, y_pred_aug, y_score_aug)\n",
    "    oof_results[(model_name, 'Real + Synthetic')] = aug_oof\n",
    "    roc_data[(model_name, 'Real + Synthetic')] = {\n",
    "        'fpr': aug_oof['roc_curve']['fpr'],\n",
    "        'tpr': aug_oof['roc_curve']['tpr'],\n",
    "        'auc': aug_oof['roc_auc'],\n",
    "    }\n",
    "\n",
    "    print(\"5-fold CV metrics (Real + Synthetic, on AUGMENTED training data):\")\n",
    "    print(\n",
    "        f\"  Mean±Std | Acc={aug_cv_sum['accuracy_mean']:.4f}±{aug_cv_sum['accuracy_std']:.4f}  \"\n",
    "        f\"Prec={aug_cv_sum['precision_mean']:.4f}±{aug_cv_sum['precision_std']:.4f}  \"\n",
    "        f\"Rec={aug_cv_sum['recall_mean']:.4f}±{aug_cv_sum['recall_std']:.4f}  \"\n",
    "        f\"F1={aug_cv_sum['f1_mean']:.4f}±{aug_cv_sum['f1_std']:.4f}  \"\n",
    "        f\"AUC={aug_cv_sum['roc_auc_mean']:.4f}±{aug_cv_sum['roc_auc_std']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nCV evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c0278",
   "metadata": {},
   "source": [
    "## 9. Comparative Metrics Table (CV Only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for model_name in MODEL_NAMES:\n",
    "    for scenario in SCENARIOS:\n",
    "        cv_sum = cv_results.get((model_name, scenario), {})\n",
    "        row = {\n",
    "            'Model': model_name.upper(),\n",
    "            'Scenario': scenario,\n",
    "            'CV_Accuracy_Mean': cv_sum.get('accuracy_mean', np.nan),\n",
    "            'CV_Accuracy_Std': cv_sum.get('accuracy_std', np.nan),\n",
    "            'CV_Precision_Mean': cv_sum.get('precision_mean', np.nan),\n",
    "            'CV_Precision_Std': cv_sum.get('precision_std', np.nan),\n",
    "            'CV_Recall_Mean': cv_sum.get('recall_mean', np.nan),\n",
    "            'CV_Recall_Std': cv_sum.get('recall_std', np.nan),\n",
    "            'CV_F1_Mean': cv_sum.get('f1_mean', np.nan),\n",
    "            'CV_F1_Std': cv_sum.get('f1_std', np.nan),\n",
    "            'CV_ROC_AUC_Mean': cv_sum.get('roc_auc_mean', np.nan),\n",
    "            'CV_ROC_AUC_Std': cv_sum.get('roc_auc_std', np.nan),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(['Model', 'Scenario']).sort_index()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"5-FOLD CV SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_df.round(4))\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "print(\"DELTA (Real + Synthetic minus Real Only) on CV MEANS:\")\n",
    "print(\"-\" * 70)\n",
    "for model_name in MODEL_NAMES:\n",
    "    a = summary_df.loc[(model_name.upper(), 'Real Only')]\n",
    "    b = summary_df.loc[(model_name.upper(), 'Real + Synthetic')]\n",
    "    print(f\"{model_name.upper()}: \", end='')\n",
    "    print(\n",
    "        f\"Acc {b['CV_Accuracy_Mean']-a['CV_Accuracy_Mean']:+.4f}, \",\n",
    "        f\"Rec {b['CV_Recall_Mean']-a['CV_Recall_Mean']:+.4f}, \",\n",
    "        f\"F1 {b['CV_F1_Mean']-a['CV_F1_Mean']:+.4f}, \",\n",
    "        f\"AUC {b['CV_ROC_AUC_Mean']-a['CV_ROC_AUC_Mean']:+.4f}\"\n",
    "    )\n",
    "print(\"-\" * 70 + \"\\n\")\n",
    "\n",
    "metrics_csv_path = '../results/metrics_comparison_all_models.csv'\n",
    "summary_df.to_csv(metrics_csv_path, index=True)\n",
    "print(f\"Saved CV metrics to: {metrics_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eea4e7",
   "metadata": {},
   "source": [
    "## 10. Plot 1: Grouped Bar Chart (CV Accuracy, Recall, F1-Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = {\n",
    "    'CV_Accuracy_Mean': 'Accuracy (CV Mean)',\n",
    "    'CV_Recall_Mean': 'Recall (CV Mean)',\n",
    "    'CV_F1_Mean': 'F1-Score (CV Mean)',\n",
    "}\n",
    "\n",
    "plot_df = summary_df.reset_index()[['Model', 'Scenario'] + list(plot_cols.keys())].copy()\n",
    "plot_df = plot_df.rename(columns=plot_cols)\n",
    "plot_df = plot_df.melt(id_vars=['Model', 'Scenario'], var_name='Metric', value_name='Score')\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df,\n",
    "    x='Model',\n",
    "    y='Score',\n",
    "    hue='Scenario',\n",
    "    col='Metric',\n",
    "    kind='bar',\n",
    "    col_wrap=3,\n",
    "    height=4,\n",
    "    aspect=1,\n",
    "    palette='Set2',\n",
    "    legend_out=True,\n",
    ")\n",
    "\n",
    "g.set(ylim=(0, 1))\n",
    "g.set_axis_labels(\"Model\", \"Score\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.fig.suptitle(\"CV Metric Comparison: Real Only vs Real + Synthetic\", y=1.05)\n",
    "\n",
    "# Add value labels\n",
    "for ax in g.axes.flatten():\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', padding=2, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98cd2f",
   "metadata": {},
   "source": [
    "## 11. Plot 2: ROC Curve Overlay (CV Out-of-Fold Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(MODEL_NAMES)\n",
    "fig, axes = plt.subplots(1, n, figsize=(6 * n, 5), sharey=True)\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "handles, labels = None, None\n",
    "for ax, model_name in zip(axes, MODEL_NAMES):\n",
    "    for scenario in SCENARIOS:\n",
    "        info = roc_data[(model_name, scenario)]\n",
    "        ax.plot(\n",
    "            info['fpr'],\n",
    "            info['tpr'],\n",
    "            linewidth=2,\n",
    "            label=f\"{scenario} (OOF AUC={info['auc']:.3f})\",\n",
    "        )\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=1)\n",
    "    ax.set_title(model_name.upper())\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.grid(True)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "fig.suptitle(\"ROC Curves (5-fold CV Out-of-Fold Predictions)\", y=1.05)\n",
    "if handles and labels:\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.02))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8983cb",
   "metadata": {},
   "source": [
    "## 12. Plot 3: Side-by-Side Confusion Matrices (CV Out-of-Fold Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(MODEL_NAMES)\n",
    "fig, axes = plt.subplots(n, 2, figsize=(12, 4 * n))\n",
    "if n == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "for i, model_name in enumerate(MODEL_NAMES):\n",
    "    for j, scenario in enumerate(SCENARIOS):\n",
    "        ax = axes[i, j]\n",
    "        cm = oof_results[(model_name, scenario)]['confusion_matrix']\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=False, text_kw={'fontsize': 12})\n",
    "        ax.set_title(f\"{model_name.upper()} | {scenario}\")\n",
    "        ax.grid(False)\n",
    "\n",
    "fig.suptitle(\"Confusion Matrices (5-fold CV Out-of-Fold Predictions)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
